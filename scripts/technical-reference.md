# Checkpoint & GPU Configuration Fixes

Technical reference for resolving common GPU and checkpoint issues when training with different hardware configurations.

## Checkpoint Device Mismatch

### Problem
```
RuntimeError: Attempting to deserialize object on CUDA device 7 
but torch.cuda.device_count() is 7.
```

### Root Cause
- Checkpoint was saved on system with 8 GPUs (devices 0-7)
- Checkpoint contains tensors that were on GPU 7
- Loading with only 7 visible GPUs (devices 0-6)
- PyTorch can't find device 7 to load the tensors

### Solution: Remap Checkpoint to CPU

**Method 1: Using Script**
```bash
python3 scripts/remap_checkpoint.py checkpoint.pth
```

**Method 2: Manual (Python)**
```python
import torch

# Load with map_location='cpu' to avoid device issues
checkpoint = torch.load('checkpoint.pth', map_location='cpu')

# Save remapped checkpoint
torch.save(checkpoint, 'checkpoint_remapped.pth')
```

### Why This Works
- `map_location='cpu'` loads all tensors to CPU regardless of original device
- PyTorch automatically moves tensors to correct GPUs during model initialization
- No data loss or performance impact
- Checkpoint becomes device-agnostic

## GPU Device Ordinal Error

### Problem
```
RuntimeError: CUDA error: invalid device ordinal
```

### Root Cause
When running multiple trainings:
1. PCDiff sets `CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6`
2. Voxelization inherits this environment variable
3. Voxelization config says `gpu: 7`
4. But GPU 7 isn't in the visible devices list
5. PyTorch throws "invalid device ordinal"

### Solution: Independent CUDA_VISIBLE_DEVICES

Each training script must explicitly set its own device visibility:

**PCDiff Script:**
```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6 torchrun --nproc_per_node=7 train.py
```

**Voxelization Script:**
```bash
export CUDA_VISIBLE_DEVICES=7  # Overrides any inherited value
python train.py config.yaml
```

### GPU Remapping

When `CUDA_VISIBLE_DEVICES=7`:
- Physical GPU 7 becomes logical device 0
- PyTorch sees only 1 GPU (device 0)
- Config should use `gpu: 0` (not `gpu: 7`)
- This maps to physical GPU 7 ‚úì

**Configuration:**
```yaml
# voxelization/configs/train_skullbreak.yaml
train:
  gpu: 0  # Logical device 0 = physical GPU 7
```

## GPU Allocation Architecture

### Physical vs Logical Mapping

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PCDiff Process                              ‚îÇ
‚îÇ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ Physical ‚Üí Logical                          ‚îÇ
‚îÇ GPU 0    ‚Üí device 0                         ‚îÇ
‚îÇ GPU 1    ‚Üí device 1                         ‚îÇ
‚îÇ GPU 2    ‚Üí device 2                         ‚îÇ
‚îÇ GPU 3    ‚Üí device 3                         ‚îÇ
‚îÇ GPU 4    ‚Üí device 4                         ‚îÇ
‚îÇ GPU 5    ‚Üí device 5                         ‚îÇ
‚îÇ GPU 6    ‚Üí device 6                         ‚îÇ
‚îÇ GPU 7    ‚Üí (hidden)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Voxelization Process                        ‚îÇ
‚îÇ CUDA_VISIBLE_DEVICES=7                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ Physical ‚Üí Logical                          ‚îÇ
‚îÇ GPU 0    ‚Üí (hidden)                         ‚îÇ
‚îÇ GPU 1    ‚Üí (hidden)                         ‚îÇ
‚îÇ GPU 2    ‚Üí (hidden)                         ‚îÇ
‚îÇ GPU 3    ‚Üí (hidden)                         ‚îÇ
‚îÇ GPU 4    ‚Üí (hidden)                         ‚îÇ
‚îÇ GPU 5    ‚Üí (hidden)                         ‚îÇ
‚îÇ GPU 6    ‚Üí (hidden)                         ‚îÇ
‚îÇ GPU 7    ‚Üí device 0                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Benefits

1. **Process Isolation**: Each process only sees its assigned GPUs
2. **No Conflicts**: Both can use "device 0" without collision
3. **Resource Control**: Clear GPU ownership per process
4. **Easy Management**: Simple environment variable control

## Learning Rate Scaling

### Linear Scaling Rule

When changing batch size, scale learning rate proportionally:

```
new_lr = old_lr √ó (new_batch / old_batch)
```

### Why Scale?

**Larger batch size:**
- Averages gradients over more samples
- Fewer optimization steps per epoch
- Each step uses more computation

**Scaling LR compensates:**
- Maintains effective learning rate
- Same convergence speed
- Similar final performance

### Examples

**PCDiff:**
```
Paper:      8 GPUs √ó 8 batch = 64,  lr = 2√ó10‚Åª‚Å¥
Your setup: 7 GPUs √ó 56 batch = 392, lr = 0.00122

Calculation: 2√ó10‚Åª‚Å¥ √ó (392/64) = 2√ó10‚Åª‚Å¥ √ó 6.125 = 0.001225 ‚âà 0.00122
```

**Voxelization:**
```
Paper:      batch = 2, lr = 5√ó10‚Åª‚Å¥
Your setup: batch = 4, lr = 1√ó10‚Åª¬≥

Calculation: 5√ó10‚Åª‚Å¥ √ó (4/2) = 5√ó10‚Åª‚Å¥ √ó 2 = 1√ó10‚Åª¬≥
```

### When to Apply

‚úÖ **Apply scaling:**
- Batch size changes are moderate (2x - 8x)
- Using well-tested optimizer (Adam, SGD)
- Training from scratch or resuming with same batch
- Goal is faster training with equivalent results

‚ö†Ô∏è **Be cautious:**
- Very large batch sizes (>1024)
- Fine-tuning pretrained models
- Batch size changes are extreme (>10x)
- Training is already unstable

### Monitoring

Watch first 100-200 epochs for:

**Good signs (LR appropriate):**
- ‚úÖ Steady loss decrease
- ‚úÖ Smooth training curves
- ‚úÖ Validation improving
- ‚úÖ No NaN/inf values

**Bad signs (LR too high):**
- ‚ùå Loss oscillates wildly
- ‚ùå Sudden spikes
- ‚ùå NaN or inf appears
- ‚ùå Training diverges

**Slow signs (LR too low):**
- üìâ Very slow progress
- üìâ Flat loss curves
- üìâ Takes many epochs to improve

## Practical Guidelines

### Before Training

1. **Remap checkpoints if needed**
   ```bash
   python3 scripts/remap_checkpoint.py checkpoint.pth
   ```

2. **Verify GPU visibility**
   ```bash
   # In PCDiff terminal
   echo $CUDA_VISIBLE_DEVICES  # Should show 0,1,2,3,4,5,6
   
   # In Voxelization terminal
   echo $CUDA_VISIBLE_DEVICES  # Should show 7
   ```

3. **Check configuration**
   ```bash
   bash scripts/setup_verify.sh
   ```

### During Training

1. **Monitor GPU usage**
   ```bash
   watch -n 1 nvidia-smi
   ```
   
   Expect:
   - GPUs 0-6: PCDiff processes (60-80% util)
   - GPU 7: Voxelization process (60-80% util)

2. **Watch for errors**
   ```bash
   tail -f pcdiff/output/*/train.log
   tail -f voxelization/out/*/train.log
   ```

3. **Track loss curves**
   - PCDiff: Should be similar to previous training
   - Voxelization: Should decrease steadily

### Troubleshooting Checklist

**Checkpoint won't load:**
- [ ] Remap checkpoint to CPU
- [ ] Verify checkpoint path exists
- [ ] Check file isn't corrupted

**GPU device errors:**
- [ ] Set CUDA_VISIBLE_DEVICES explicitly in script
- [ ] Update config to use logical device 0
- [ ] Verify no other processes using GPUs

**Training unstable:**
- [ ] Reduce learning rate by 30-50%
- [ ] Check batch size isn't too large
- [ ] Verify data loading works correctly
- [ ] Monitor GPU memory usage

**Slow training:**
- [ ] Increase learning rate (if stable)
- [ ] Check GPU utilization is high
- [ ] Verify not CPU-bound (data loading)
- [ ] Consider increasing batch size

## References

### Research Papers

- Goyal et al. 2017: "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"
- You et al. 2017: "Large Batch Training of Convolutional Networks"  
- Smith et al. 2018: "Don't Decay the Learning Rate, Increase the Batch Size"

### PyTorch Documentation

- [CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html)
- [Distributed Training](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [Saving & Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html)

---

For practical usage, see [training-guide.md](training-guide.md)

